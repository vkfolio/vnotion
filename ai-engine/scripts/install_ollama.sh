#!/bin/bash
# Install Ollama and recommended models for VNotions AI Engine

set -e

echo "ðŸ¦™ Installing Ollama for VNotions AI Engine..."

# Check if Ollama is already installed
if command -v ollama &> /dev/null; then
    echo "âœ“ Ollama is already installed"
else
    echo "ðŸ“¦ Installing Ollama..."
    curl -fsSL https://ollama.ai/install.sh | sh
    echo "âœ“ Ollama installed successfully"
fi

# Start Ollama service
echo "ðŸš€ Starting Ollama service..."
if pgrep -x "ollama" > /dev/null; then
    echo "âœ“ Ollama is already running"
else
    # Start Ollama in background
    ollama serve &
    OLLAMA_PID=$!
    echo "âœ“ Ollama started with PID: $OLLAMA_PID"
    
    # Wait for Ollama to be ready
    echo "â³ Waiting for Ollama to be ready..."
    sleep 5
    
    # Check if Ollama is responding
    for i in {1..30}; do
        if curl -s http://localhost:11434/api/version > /dev/null; then
            echo "âœ“ Ollama is ready"
            break
        fi
        echo "   Waiting... ($i/30)"
        sleep 2
    done
fi

# Install recommended models
echo "ðŸ“š Installing recommended AI models..."

MODELS=("llama2" "mistral" "codellama")

for model in "${MODELS[@]}"; do
    echo "ðŸ“¥ Installing $model..."
    if ollama list | grep -q "$model"; then
        echo "âœ“ $model is already installed"
    else
        ollama pull "$model"
        echo "âœ“ $model installed successfully"
    fi
done

echo ""
echo "ðŸŽ‰ Installation complete!"
echo ""
echo "ðŸ“‹ Installed models:"
ollama list

echo ""
echo "ðŸ”§ Next steps:"
echo "1. cd to your ai-engine directory"
echo "2. Install Python dependencies: pip install -r requirements.txt"
echo "3. Start the AI Engine: python start.py"
echo ""
echo "ðŸ“– For more information, see README.md"